# Bontle - Advanced AI Assistant with Dual STT, Voice Signatures & Interrupt TTS

An intelligent AI assistant featuring dual speech recognition engines (Kyutai STT + RealTimeSTT), advanced voice recognition, interruptible text-to-speech, and seamless real-time interaction capabilities.

## ‚ú® Latest Features

### üéØ **NEW: Kyutai STT Integration** (v4.0.0)
Revolutionary dual STT system with state-of-the-art Kyutai STT-2.6B and intelligent fallback:
- **Kyutai STT-2.6B-EN**: State-of-the-art speech recognition with 2.6B parameters
- **GPU Acceleration**: Optimized for NVIDIA GPUs and Apple Silicon (MPS)
- **Intelligent Fallback**: Automatic fallback to RealTimeSTT for reliability
- **Engine Selection**: Choose between high-quality and real-time transcription
- **Unified Management**: Single interface for multiple STT engines
- **Advanced Configuration**: Comprehensive settings for optimal performance

### üîÑ **TTS with Interrupt** (v3.0.0)
Revolutionary interruptible text-to-speech system that allows natural conversation flow:
- **Instant Interruption**: Say hotwords during TTS playback to interrupt and give new commands immediately
- **Natural Conversation**: No more waiting for responses to finish - just like talking to a human
- **Seamless Integration**: Uses same audio from hotword detection for speaker identification
- **Thread-Safe**: Robust interrupt handling with proper resource cleanup
- **Dual TTS Support**: Primary Sesame CSM (local, GPU-accelerated) with OpenAI fallback

### üéôÔ∏è **Enhanced STT System**
- **Kyutai STT-2.6B-EN**: High-quality, GPU-accelerated speech recognition with 2.5s delay
- **RealTimeSTT Integration**: Real-time speech recognition with minimal delay
- **Intelligent Fallback**: Automatic engine switching for reliability
- **GPU Optimization**: CUDA and MPS acceleration for maximum performance
- **Engine Selection**: Runtime switching between quality and speed modes
- **Batch Processing**: Efficient processing of multiple audio files

### ÔøΩ **Enhanced TTS System**
- **Sesame CSM TTS**: Local, high-quality text-to-speech with GPU acceleration
- **Model Preloading**: Instant response times with preloaded models
- **Smart Fallback**: Automatic fallback to OpenAI TTS if CSM fails
- **Interrupt Capability**: All TTS engines support immediate interruption

### üéØ **Real-time Speech Integration**
- **Dual STT Engines**: Kyutai STT (quality) + RealTimeSTT (speed)
- **Audio Chunk Reuse**: Uses hotword detection audio for speaker identification (no additional delays)
- **GPU Optimization**: All models load on GPU for maximum performance
- **Minimal Latency**: Optimized for instant response times

## Core Features

### AI Conversation System
- AI-powered conversations using Ollama
- Speaker-aware conversation history
- Real-time speech processing with hotword detection
- Weather information and tool integration
- Spotify integration and image searching capabilities

### Voice Signature Recognition üé§
Advanced speaker identification system that recognizes who is talking based on their unique voice characteristics:

#### Voice Signature Capabilities:
- **Speaker Registration**: Register multiple users with their unique voice signatures
- **Real-time Speaker Identification**: Automatically identify speakers during conversations
- **Speaker-aware Conversations**: Maintain context and personalization based on who is speaking
- **Multi-speaker Support**: Handle conversations with multiple registered users
- **Voice Database Management**: Add, remove, and manage registered speakers

#### How Voice Signature Works:
1. **Feature Extraction**: Analyzes voice characteristics including:
   - Spectral features (frequency distribution, brightness)
   - Prosodic features (pitch, rhythm, energy)
   - Temporal patterns (speaking rate, pauses)
   - Formant features (vocal tract characteristics)

2. **Speaker Registration**: Users record a 5-second voice sample that gets processed and stored as a unique voice signature

3. **Speaker Identification**: During conversations, the system:
   - Records 3-second voice samples
   - Extracts voice features
   - Compares against registered signatures
   - Identifies the speaker with confidence scoring

4. **Conversation Integration**: Identified speakers are automatically tagged in conversation history for personalized responses

## Project Structure

```
Bontle/
‚îú‚îÄ‚îÄ assist.py                    # Main assistant with TTS interrupt functionality
‚îú‚îÄ‚îÄ jarvis.py                    # Real-time speech processing with RealTimeSTT
‚îú‚îÄ‚îÄ jarvis_enhanced.py           # Enhanced assistant with dual STT engines
‚îú‚îÄ‚îÄ kyutai_stt.py               # Kyutai STT-2.6B implementation with GPU optimization
‚îú‚îÄ‚îÄ stt_config.py               # STT engine configuration management
‚îú‚îÄ‚îÄ unified_stt.py              # Unified STT manager with intelligent fallback
‚îú‚îÄ‚îÄ csm_tts.py                  # Sesame CSM TTS implementation with GPU optimization
‚îú‚îÄ‚îÄ voice_signature.py          # Voice signature recognition system
‚îú‚îÄ‚îÄ spot.py                     # Spotify integration
‚îú‚îÄ‚îÄ tools.py                    # Utility functions
‚îú‚îÄ‚îÄ test_interrupt.py           # TTS interrupt functionality testing
‚îú‚îÄ‚îÄ test_kyutai_stt.py          # Comprehensive Kyutai STT testing
‚îú‚îÄ‚îÄ requirements.txt            # Python dependencies
‚îú‚îÄ‚îÄ voice_signatures.json       # Voice signature database
‚îú‚îÄ‚îÄ voice_recordings/           # Stored voice samples
‚îú‚îÄ‚îÄ KYUTAI_STT_SETUP_GUIDE.md   # Comprehensive Kyutai STT setup guide
‚îú‚îÄ‚îÄ TTS_INTERRUPT_GUIDE.md      # Complete TTS interrupt documentation
‚îú‚îÄ‚îÄ CSM_SETUP_GUIDE.md         # CSM TTS setup and configuration
‚îú‚îÄ‚îÄ GPU_OPTIMIZATION_SUMMARY.md # GPU optimization details
‚îî‚îÄ‚îÄ README.md
```

### Architecture Highlights:
- **Modular Design**: Each component is separate and focused on specific functionality
- **Dual STT System**: Kyutai STT (quality) + RealTimeSTT (speed) with intelligent selection
- **Thread-Safe**: Interrupt system uses proper threading mechanisms
- **GPU Optimized**: All models prioritize GPU usage for maximum performance
- **Fallback Support**: Robust error handling with automatic fallbacks
- **Real-time Processing**: Optimized for minimal latency and instant responses

## Installation

1. Clone the repository
2. Create a virtual environment:
   ```bash
   python -m venv .venv
   .venv\Scripts\activate  # On Windows
   source .venv/bin/activate  # On Linux/Mac
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

4. **For Kyutai STT support**, ensure you have the latest transformers:
   ```bash
   pip install transformers>=4.53.0
   ```

5. Create a `.env` file with your API keys and STT configuration:
   ```
   OPENAI_API_KEY=your_openai_api_key_here
   HUGGINGFACE_TOKEN=your_hf_token_here
   SYSTEM_PROMPT=your_system_prompt_here
   OLLAMA_BASE_URL=http://localhost:11434  # Optional: if using custom Ollama setup
   
   # STT Configuration
   STT_ENGINE=kyutai                    # Options: kyutai, realtime
   STT_USE_GPU=True                     # Enable GPU acceleration
   STT_FALLBACK_ENABLED=True            # Enable automatic fallback
   ```

6. Install PyTorch with CUDA support (for GPU acceleration):
   ```bash
   # For CUDA 11.8
   pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118
   
   # For CUDA 12.1
   pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu121
   ```

7. **Test your Kyutai STT setup**:
   ```bash
   python test_kyutai_stt.py
   ```

## Quick Start

### Enhanced Assistant with Dual STT (Recommended)
```bash
# Start the enhanced assistant with Kyutai STT + RealTimeSTT
python jarvis_enhanced.py
```

**Features:**
- üéØ **Dual STT Engines**: Choose between Kyutai STT (quality) and RealTimeSTT (speed)
- üîÑ **Intelligent Fallback**: Automatic engine switching for reliability
- ‚ö° **GPU Acceleration**: Optimized Kyutai STT on CUDA/MPS
- üéôÔ∏è **Interrupt TTS**: Say hotwords during playback to interrupt immediately
- üë§ **Speaker Recognition**: Automatic speaker identification using captured audio
- üöÄ **Model Preloading**: All models preloaded for instant responses

### Real-time Assistant (RealTimeSTT Primary)
```bash
# Start the main assistant with continuous hotword detection
python jarvis.py
```

**Features:**
- üéôÔ∏è Continuous listening for hotwords ("Bontle", "Jarvis", "Hi")
- üîÑ Interrupt TTS by saying hotwords during playback
- üë§ Automatic speaker identification using hotword audio
- ‚ö° GPU-accelerated CSM TTS for high-quality speech
- üöÄ Model preloading for instant responses

### Interactive Assistant (Text-based)
```bash
# Run text-based assistant with voice recognition setup
python assist.py
```

## TTS with Interrupt System

### How It Works
1. **Natural Conversation**: Say "Bontle" to start a conversation
2. **Interrupt Anytime**: Say "Bontle" again during TTS playback to interrupt and give new commands
3. **Seamless Transition**: No delays - immediate response to interruption
4. **Smart Audio Reuse**: Uses the same audio for both hotword detection and speaker identification

### Example Usage
```python
from assist import TTS_with_interrupt, interrupt_tts, is_tts_active

# Start interruptible TTS
result = TTS_with_interrupt("This is a long message that can be interrupted...")

# Check status
if result == "interrupted":
    print("TTS was interrupted by user")
elif result == "done":
    print("TTS completed successfully")
elif result == "error":
    print("TTS failed, check logs")

# Programmatically interrupt TTS
if is_tts_active():
    interrupt_tts()
```

### Testing Interrupt Functionality
```bash
# Run comprehensive interrupt tests
python test_interrupt.py
```

## Usage

### Advanced TTS System
```python
from assist import TTS_with_interrupt, TTS, csm_text_to_speech

# Interruptible TTS (recommended for real-time applications)
result = TTS_with_interrupt("Hello, this message can be interrupted.")

# Standard TTS with CSM + OpenAI fallback
TTS("Hello, this is a standard TTS message.")

# Direct CSM TTS usage
csm_text_to_speech("Hello from Sesame CSM!", play_audio=True)
```

### Real-time Speech Processing
```python
from assist import TTS_with_interrupt, TTS, csm_text_to_speech

# Interruptible TTS (recommended for real-time applications)
result = TTS_with_interrupt("Hello, this message can be interrupted.")

# Standard TTS with CSM + OpenAI fallback
TTS("Hello, this is a standard TTS message.")

# Direct CSM TTS usage
csm_text_to_speech("Hello from Sesame CSM!", play_audio=True)
```

### Dual STT System
```python
from unified_stt import get_unified_stt, switch_stt_engine, get_stt_status

# Get STT status
status = get_stt_status()
print(f"Active Engine: {status['active_engine']}")
print(f"Available Engines: {status['kyutai_available']}, {status['realtime_available']}")

# Switch engines at runtime
switch_stt_engine('kyutai')    # High quality, 2.5s delay
switch_stt_engine('realtime')  # Real-time, minimal delay

# Transcribe audio with automatic engine selection
stt = get_unified_stt()
text = stt.transcribe_audio_file("recording.wav")
```

### Kyutai STT Direct Usage
```python
from kyutai_stt import transcribe_with_kyutai

# High-quality transcription
text = transcribe_with_kyutai(audio_path="recording.wav")

# Or with numpy array
text = transcribe_with_kyutai(audio_array=audio_data)

# Batch processing for efficiency
from kyutai_stt import initialize_kyutai_stt
stt = initialize_kyutai_stt()
results = stt.batch_transcribe([audio1, audio2, audio3])
```

### Basic Assistant Functions
```python
from assist import ask_question_memory

# Ask a question with speaker identification
response = ask_question_memory("Hello, how are you?", identify_speaker=True)
print(response)
```

### Voice Signature Recognition

#### Using the Voice Signature Module
```python
# Import voice signature functions
from voice_signature import (
    register_new_speaker, 
    identify_current_speaker, 
    get_speaker_info,
    voice_manager
)

# Register a new speaker (will prompt for 5-second recording)
success = register_new_speaker("John Doe", duration=5)

# Identify current speaker (will prompt for 3-second recording)
speaker_name, confidence = identify_current_speaker(duration=3)
print(f"Speaker: {speaker_name} (confidence: {confidence:.2f})")

# Get information about registered speakers
print(get_speaker_info())

# Access the voice manager directly for advanced operations
speakers = voice_manager.list_registered_speakers()
```

#### Speaker-Aware Conversation
```python
from assist import ask_question_memory

# Ask question with automatic speaker identification
response = ask_question_memory("What's the weather like?", identify_speaker=True)
```

#### Manage Voice Database
```python
from assist import list_all_speakers, remove_registered_speaker, get_speaker_info

# List all registered speakers
speakers = list_all_speakers()
print(f"Registered speakers: {speakers}")

# Get speaker information
info = get_speaker_info()
print(info)

# Remove a speaker
remove_registered_speaker("John Doe")
```

### Interactive Assistant Demo
Run the main assistant with real-time speech and interrupt capabilities:

```bash
# Full-featured assistant with hotword detection
python jarvis.py
```

**Features in jarvis.py:**
- üéØ Continuous hotword detection ("Bontle", "Jarvis", "Hi")
- üîÑ Interrupt TTS playback with new hotwords
- üë§ Automatic speaker identification using captured audio
- ‚ö° GPU-accelerated model preloading
- üéôÔ∏è High-quality CSM TTS with OpenAI fallback
- üöÄ Optimized for minimal latency

```bash
# Text-based assistant with voice setup
python assist.py
```

**Features in assist.py:**
- üí¨ Text-based conversation interface
- üé§ Voice signature setup and management
- üîß Interactive voice settings menu
- üìä Speaker database management

### Voice Signature Management (Standalone)
Run the voice signature module independently for setup and testing:

```bash
python voice_signature.py
```

This launches an interactive menu with options to:
1. Register new speakers
2. Identify speakers  
3. List registered speakers
4. Remove speakers
5. Update speaker features
6. Demo conversation with speaker identification

### Command Line Options
```bash
# Run assistant with voice settings menu
python assist.py --voice-settings

# Run main assistant loop (default)
python assist.py
```

## Technical Architecture

### TTS Interrupt System
- **Thread-Safe Design**: Uses `threading.Event` for safe interrupt signaling
- **Resource Management**: Proper cleanup of audio files and pygame resources
- **Status Tracking**: Real-time monitoring of TTS playback state
- **Fallback Support**: CSM ‚Üí OpenAI TTS with interrupt support in both

### GPU Optimization
- **Model Preloading**: CSM TTS and RealtimeSTT models preloaded on GPU
- **Device Priority**: Automatic GPU detection and usage
- **Memory Management**: Efficient GPU memory usage with proper cleanup
- **Performance Monitoring**: Device usage tracking and optimization

### Real-time Audio Pipeline
```
Microphone ‚Üí RealtimeSTT ‚Üí Hotword Detection ‚Üí Speaker ID ‚Üí Conversation ‚Üí CSM TTS ‚Üí Audio Output
                              ‚Üì
                          Interrupt Signal ‚Üí Stop TTS ‚Üí Process New Command
```

### File Structure

```
Bontle/
‚îú‚îÄ‚îÄ assist.py                    # Core conversation logic with TTS interrupt
‚îú‚îÄ‚îÄ jarvis.py                    # Real-time speech processing main loop
‚îú‚îÄ‚îÄ csm_tts.py                   # Sesame CSM TTS with GPU optimization
‚îú‚îÄ‚îÄ voice_signature.py           # Voice recognition and speaker identification
‚îú‚îÄ‚îÄ tools.py                     # Utility functions and integrations
‚îú‚îÄ‚îÄ test_interrupt.py            # TTS interrupt testing framework
‚îú‚îÄ‚îÄ requirements.txt             # All dependencies
‚îú‚îÄ‚îÄ .env                         # Configuration and API keys
‚îú‚îÄ‚îÄ voice_signatures.json        # Speaker database
‚îú‚îÄ‚îÄ voice_recordings/            # Audio samples storage
‚îú‚îÄ‚îÄ TTS_INTERRUPT_GUIDE.md       # Complete interrupt documentation
‚îú‚îÄ‚îÄ CSM_SETUP_GUIDE.md          # CSM installation and setup
‚îú‚îÄ‚îÄ GPU_OPTIMIZATION_SUMMARY.md  # Performance optimization guide
‚îî‚îÄ‚îÄ README.md                    # This file
```

## Dependencies

### Core Dependencies
- `openai` - OpenAI API for fallback TTS
- `ollama` - Local AI conversation model
- `pygame` - Audio playback and mixing
- `python-dotenv` - Environment variable management

### STT and Speech Processing
- `torch` - PyTorch for GPU acceleration
- `torchaudio` - Audio processing with PyTorch
- `transformers>=4.53.0` - Hugging Face models (Kyutai STT support)
- `soundfile` - Audio file I/O
- `RealtimeSTT` - Real-time speech recognition
- `scipy` - Signal processing for audio resampling

### Voice Signature Dependencies
- `numpy` - Numerical computations
- `scipy` - Signal processing and audio analysis
- `librosa` - Advanced audio feature extraction
- `scikit-learn` - Machine learning utilities
- `pyaudio` - Audio recording and streaming

### Optional Dependencies
- `accelerate` - Hugging Face model acceleration
- `optimum` - Model optimization
- `onnxruntime` - ONNX runtime for optimized inference

## Configuration

### Environment Variables
Create a `.env` file with:
```env
# Required
OPENAI_API_KEY=your_openai_api_key
SYSTEM_PROMPT=Your custom system prompt for conversation context

# Optional
OLLAMA_BASE_URL=http://localhost:11434
HF_HOME=./models  # Hugging Face model cache directory
CUDA_VISIBLE_DEVICES=0  # GPU device selection
```

### TTS Configuration
Modify settings in `assist.py`:
```python
# TTS Settings
USE_LOCAL_CSM_TTS = True  # Use CSM as primary, OpenAI as fallback

# Interrupt System Settings
tts_interrupt_flag = threading.Event()  # Thread-safe interrupt signal
tts_playback_active = threading.Event()  # Playback state tracking
```

### CSM TTS Settings
Modify settings in `csm_tts.py`:
```python
# Model Configuration
MODEL_NAME = "facebook/fastspeech2-en-ljspeech"  # CSM model
SAMPLE_RATE = 22050  # Audio sample rate
DEVICE_PRIORITY = ["cuda", "cpu"]  # Device selection priority

# Performance Settings
PRELOAD_MODEL = True  # Preload model for instant generation
GPU_MEMORY_FRACTION = 0.8  # GPU memory allocation
```

### Voice Signature Settings
Modify settings in `voice_signature.py`:
```python
# Audio Recording Parameters
SAMPLE_RATE = 16000      # Sample rate for recordings
CHANNELS = 1             # Mono audio
REGISTRATION_DURATION = 5 # Seconds for speaker registration
IDENTIFICATION_DURATION = 3 # Seconds for speaker identification
SIMILARITY_THRESHOLD = 0.7 # Minimum confidence for positive ID

# Feature Extraction Settings
N_MFCC = 40             # Number of MFCC coefficients
N_FFT = 2048            # FFT window size
HOP_LENGTH = 512        # Hop length for analysis
```

### RealtimeSTT Settings
Modify settings in `jarvis.py`:
```python
# Speech Recognition Configuration
recorder = AudioToTextRecorder(
    model="medium.en",  # Whisper model size
    language="en",      # Language code
    post_speech_silence_duration=0.15,  # Silence detection
    silero_sensitivity=0.4,  # Voice activity detection
    enable_realtime_transcription=False  # Disable real-time for better accuracy
)

# Hotword Configuration
hot_words = ["bontle", "jarvis", "hi"]  # Activation words
```

## Advanced Usage

### Custom TTS Integration
```python
from csm_tts import CSMTextToSpeech

# Direct CSM TTS usage with custom settings
tts = CSMTextToSpeech()
audio = tts.generate_audio("Custom text", temperature=0.9)
tts.play_audio(audio)
```

### Interrupt System Integration
```python
import assist
import threading
import time

def background_tts():
    """Run TTS in background thread"""
    result = assist.TTS_with_interrupt("This is a long message...")
    return result

# Start TTS in background
tts_thread = threading.Thread(target=background_tts)
tts_thread.start()

# Interrupt after delay
time.sleep(2)
if assist.is_tts_active():
    assist.interrupt_tts()
    print("TTS interrupted successfully")
```

### Real-time Speech Integration
```python
from RealtimeSTT import AudioToTextRecorder
import assist

def on_audio_chunk(chunk):
    """Capture audio for speaker identification"""
    global captured_audio
    captured_audio = chunk

def main_loop():
    recorder = AudioToTextRecorder(on_recorded_chunk=on_audio_chunk)
    
    while True:
        text = recorder.text()
        
        # Check for interrupt during TTS
        if assist.is_tts_active() and "bontle" in text.lower():
            assist.interrupt_tts()
        
        # Process with speaker ID using captured audio
        if "bontle" in text.lower():
            response = assist.ask_question_memory(
                text, 
                identify_speaker=True, 
                audio_file=captured_audio
            )
            assist.TTS_with_interrupt(response)
```

### Voice Database Management
```python
from voice_signature import voice_manager

# Advanced voice management
speakers = voice_manager.list_registered_speakers()
for speaker in speakers:
    info = voice_manager.get_speaker_info(speaker)
    print(f"Speaker: {speaker}, Confidence: {info.get('avg_confidence', 'N/A')}")

# Update speaker features
voice_manager.update_speaker_features("John Doe")

# Batch speaker identification
results = voice_manager.batch_identify(audio_files)
```

## Troubleshooting

### TTS Issues

1. **CSM TTS Not Working**
   - Check GPU availability: `python -c "import torch; print(torch.cuda.is_available())"`
   - Verify PyTorch installation with CUDA support
   - Check internet connection for model download
   - Review logs for specific error messages

2. **TTS Interrupt Not Working**
   - Verify threading imports in assist.py
   - Check that jarvis.py calls `interrupt_tts()` properly
   - Test with `python test_interrupt.py`
   - Ensure pygame mixer is initialized

3. **Audio Playback Problems**
   - Check audio device availability
   - Verify pygame installation: `pip install pygame`
   - Test with different audio formats
   - Check system audio settings

### Speech Recognition Issues

1. **RealtimeSTT Problems**
   - Ensure microphone permissions are granted
   - Check PyAudio installation: `pip install pyaudio`
   - Verify CUDA support for Whisper models
   - Test with different Whisper model sizes

2. **Hotword Detection Issues**
   - Adjust `silero_sensitivity` in jarvis.py
   - Check microphone input levels
   - Test with different hotwords
   - Verify post_speech_silence_duration settings

### Voice Signature Issues

1. **Speaker Recognition Accuracy**
   - Use quiet recording environment
   - Ensure consistent microphone distance
   - Re-register speakers with better quality samples
   - Adjust similarity threshold (default: 0.7)

2. **Audio Recording Problems**
   - Check microphone permissions
   - Verify audio device availability
   - Test recording duration (minimum 1 second)
   - Check scipy installation for signal processing

### GPU and Performance Issues

1. **GPU Not Being Used**
   - Install CUDA-compatible PyTorch: `pip install torch --index-url https://download.pytorch.org/whl/cu118`
   - Check CUDA installation: `nvidia-smi`
   - Verify GPU memory availability
   - Check device selection in csm_tts.py

2. **Model Loading Issues**
   - Check internet connection for Hugging Face downloads
   - Verify HF_HOME directory permissions
   - Clear model cache if corrupted: `rm -rf ~/.cache/huggingface`
   - Check disk space for model storage

3. **Performance Optimization**
   - Preload models with `preload_csm_model()`
   - Use appropriate Whisper model size for your hardware
   - Adjust GPU memory allocation settings
   - Monitor system resources during operation

### Common Error Messages

**"CUDA out of memory"**
- Reduce model size or restart Python session
- Adjust GPU_MEMORY_FRACTION in csm_tts.py
- Close other GPU-using applications

**"No module named 'transformers'"**
- Install missing dependencies: `pip install transformers`
- Check virtual environment activation

**"Audio device not found"**
- Check system audio settings
- Verify microphone connection
- Test with different audio devices

**"Model download failed"**
- Check internet connection
- Verify Hugging Face access
- Try manual model download

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test voice signature functionality
5. Submit a pull request

## License

This project is licensed under the MIT License.

## Changelog

### v4.0.0 - Kyutai STT Integration & Dual Engine System (September 2025)
- üéØ **Kyutai STT-2.6B-EN**: State-of-the-art speech recognition with 2.6B parameters
- üîÑ **Dual STT System**: Intelligent choice between Kyutai STT and RealTimeSTT
- ‚ö° **GPU Acceleration**: CUDA and Apple MPS optimization for Kyutai STT
- üéõÔ∏è **Unified STT Manager**: Single interface for multiple STT engines
- üîÑ **Intelligent Fallback**: Automatic engine switching for reliability
- ‚öôÔ∏è **Advanced Configuration**: Comprehensive STT settings and optimization
- üß™ **Testing Suite**: Complete test framework for STT validation
- üìö **Enhanced Documentation**: Detailed setup guides and usage examples

### v3.0.0 - TTS with Interrupt & Real-time Speech (September 2025)
- ‚ú® **TTS Interrupt System**: Revolutionary interruptible text-to-speech
- üéôÔ∏è **Sesame CSM TTS**: Local, GPU-accelerated high-quality TTS
- üîÑ **Real-time Integration**: Seamless RealtimeSTT integration with audio reuse
- ‚ö° **GPU Optimization**: All models preloaded on GPU for instant responses
- üéØ **Hotword Detection**: Continuous listening with interrupt capability
- üßµ **Thread-Safe Design**: Robust interrupt handling with proper cleanup
- üìä **Smart Fallback**: CSM ‚Üí OpenAI TTS with interrupt support in both
- üöÄ **Performance**: Minimal latency with optimized model loading

### v2.0.0 - Voice Signature Recognition (Previous)
- üë§ Added complete voice signature recognition system
- üé§ Speaker registration and identification
- üí¨ Speaker-aware conversation tracking
- üîß Interactive demo and management tools
- üìà Comprehensive voice feature extraction
- üìÅ Voice database management

### v1.0.0 - Initial Release
- ü§ñ Basic AI assistant functionality
- üó£Ô∏è Text-to-Speech integration
- üí≠ Ollama conversation support

## Performance Benchmarks

### STT Engine Comparison (GPU)
| Engine | Accuracy | Speed | GPU Usage | Real-time | Best Use Case |
|--------|----------|-------|-----------|-----------|---------------|
| **Kyutai STT** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | High-quality transcription |
| **RealTimeSTT** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Interactive conversations |

### STT Response Times (GPU)
- **Kyutai STT Processing**: ~2.5 seconds (with 2.5s model delay)
- **RealTimeSTT Processing**: ~0.2-0.5 seconds
- **Engine Switching**: <0.1 seconds
- **Model Loading Time**: ~5-10 seconds (one-time at startup)

### TTS Response Times (GPU)
- **CSM TTS Generation**: ~0.5-1.0 seconds
- **OpenAI TTS Fallback**: ~1.0-2.0 seconds
- **Interrupt Latency**: <0.1 seconds
- **Model Preload Time**: ~3-5 seconds (one-time)

### Speech Recognition Performance
- **Hotword Detection**: <0.2 seconds
- **Speaker Identification**: ~0.3-0.5 seconds
- **Audio Chunk Reuse**: No additional delay
- **Unified STT Fallback**: ~0.5-1.0 seconds additional

### System Requirements
- **Minimum**: 8GB RAM, Intel i5 or equivalent
- **Recommended**: 16GB RAM, NVIDIA GPU with 6GB+ VRAM
- **Optimal**: 32GB RAM, NVIDIA RTX 3080+ or better
- **Storage**: 3-7GB for models (cached locally)

## Documentation

### Complete Guides
- üìñ **[KYUTAI_STT_SETUP_GUIDE.md](KYUTAI_STT_SETUP_GUIDE.md)** - Complete Kyutai STT setup and configuration
- üìñ **[TTS_INTERRUPT_GUIDE.md](TTS_INTERRUPT_GUIDE.md)** - Complete TTS interrupt implementation guide
- üîß **[CSM_SETUP_GUIDE.md](CSM_SETUP_GUIDE.md)** - Sesame CSM setup and configuration
- ‚ö° **[GPU_OPTIMIZATION_SUMMARY.md](GPU_OPTIMIZATION_SUMMARY.md)** - Performance optimization guide
- üìù **[README.md](README.md)** - This comprehensive overview

### Quick Reference
- üéôÔ∏è **Enhanced Assistant**: `python jarvis_enhanced.py` (dual STT with engine choice)
- üé§ **Real-time Assistant**: `python jarvis.py` (RealTimeSTT with interrupts)
- üí¨ **Text Assistant**: `python assist.py` (text-based with voice setup)
- üß™ **Test Kyutai STT**: `python test_kyutai_stt.py` (verify STT setup)
- üß™ **Test Interrupts**: `python test_interrupt.py` (verify interrupt functionality)
- üîß **Voice Setup**: `python assist.py --voice-settings` (voice management)

### API Reference
- **`unified_stt.transcribe_audio(audio_path)`** - Unified STT with automatic engine selection
- **`kyutai_stt.transcribe_with_kyutai(audio_path)`** - Direct Kyutai STT usage
- **`switch_stt_engine('kyutai'|'realtime')`** - Runtime engine switching
- **`assist.TTS_with_interrupt(text)`** - Interruptible TTS with status return
- **`assist.interrupt_tts()`** - Immediately stop active TTS playback
- **`assist.is_tts_active()`** - Check if TTS is currently playing
- **`voice_manager.identify_speaker(audio_path)`** - Speaker identification

## Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Make your changes with proper documentation
4. Test thoroughly, especially TTS interrupt functionality
5. Run test suite: `python test_interrupt.py`
6. Commit changes: `git commit -m 'Add amazing feature'`
7. Push to branch: `git push origin feature/amazing-feature`
8. Submit a pull request

### Development Guidelines
- Follow existing code style and patterns
- Add comprehensive docstrings for new functions
- Test interrupt functionality thoroughly
- Update documentation for new features
- Maintain backward compatibility where possible

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Support & Contact

- üêõ **Issues**: Report bugs via GitHub Issues
- üí° **Feature Requests**: Submit enhancement ideas
- üìß **Contact**: Reach out via GitHub or email
- üìö **Documentation**: Check the complete guides in the repository

## Acknowledgments

- **Hugging Face**: For the Sesame CSM TTS model
- **OpenAI**: For GPT models and TTS API
- **RealtimeSTT**: For real-time speech recognition
- **Community**: For feedback and contributions

---

**Bontle v3.0.0** - Advanced AI Assistant with Revolutionary Interrupt TTS System

*Experience natural, interruptible conversations with your AI assistant!* üöÄ
