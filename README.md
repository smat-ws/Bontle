# Bontle - Advanced AI Assistant with Voice Signature Recognition & Interrupt TTS

An intelligent AI assistant featuring advanced voice recognition, interruptible text-to-speech, and seamless real-time interaction capabilities.

## ‚ú® Latest Features

### üîÑ **NEW: TTS with Interrupt** (v3.0.0)
Revolutionary interruptible text-to-speech system that allows natural conversation flow:
- **Instant Interruption**: Say hotwords during TTS playback to interrupt and give new commands immediately
- **Natural Conversation**: No more waiting for responses to finish - just like talking to a human
- **Seamless Integration**: Uses same audio from hotword detection for speaker identification
- **Thread-Safe**: Robust interrupt handling with proper resource cleanup
- **Dual TTS Support**: Primary Sesame CSM (local, GPU-accelerated) with OpenAI fallback

### üéôÔ∏è **Enhanced TTS System**
- **Sesame CSM TTS**: Local, high-quality text-to-speech with GPU acceleration
- **Model Preloading**: Instant response times with preloaded models
- **Smart Fallback**: Automatic fallback to OpenAI TTS if CSM fails
- **Interrupt Capability**: All TTS engines support immediate interruption

### üéØ **Real-time Speech Integration**
- **RealtimeSTT**: Continuous hotword detection and speech recognition
- **Audio Chunk Reuse**: Uses hotword detection audio for speaker identification (no additional delays)
- **GPU Optimization**: All models load on GPU for maximum performance
- **Minimal Latency**: Optimized for instant response times

## Core Features

### AI Conversation System
- AI-powered conversations using Ollama
- Speaker-aware conversation history
- Real-time speech processing with hotword detection
- Weather information and tool integration
- Spotify integration and image searching capabilities

### Voice Signature Recognition üé§
Advanced speaker identification system that recognizes who is talking based on their unique voice characteristics:

#### Voice Signature Capabilities:
- **Speaker Registration**: Register multiple users with their unique voice signatures
- **Real-time Speaker Identification**: Automatically identify speakers during conversations
- **Speaker-aware Conversations**: Maintain context and personalization based on who is speaking
- **Multi-speaker Support**: Handle conversations with multiple registered users
- **Voice Database Management**: Add, remove, and manage registered speakers

#### How Voice Signature Works:
1. **Feature Extraction**: Analyzes voice characteristics including:
   - Spectral features (frequency distribution, brightness)
   - Prosodic features (pitch, rhythm, energy)
   - Temporal patterns (speaking rate, pauses)
   - Formant features (vocal tract characteristics)

2. **Speaker Registration**: Users record a 5-second voice sample that gets processed and stored as a unique voice signature

3. **Speaker Identification**: During conversations, the system:
   - Records 3-second voice samples
   - Extracts voice features
   - Compares against registered signatures
   - Identifies the speaker with confidence scoring

4. **Conversation Integration**: Identified speakers are automatically tagged in conversation history for personalized responses

## Project Structure

```
Bontle/
‚îú‚îÄ‚îÄ assist.py                    # Main assistant with TTS interrupt functionality
‚îú‚îÄ‚îÄ jarvis.py                    # Real-time speech processing and hotword detection
‚îú‚îÄ‚îÄ csm_tts.py                   # Sesame CSM TTS implementation with GPU optimization
‚îú‚îÄ‚îÄ voice_signature.py           # Voice signature recognition system
‚îú‚îÄ‚îÄ spot.py                      # Spotify integration
‚îú‚îÄ‚îÄ tools.py                     # Utility functions
‚îú‚îÄ‚îÄ test_interrupt.py            # TTS interrupt functionality testing
‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies
‚îú‚îÄ‚îÄ voice_signatures.json        # Voice signature database
‚îú‚îÄ‚îÄ voice_recordings/            # Stored voice samples
‚îú‚îÄ‚îÄ TTS_INTERRUPT_GUIDE.md       # Complete TTS interrupt documentation
‚îú‚îÄ‚îÄ CSM_SETUP_GUIDE.md          # CSM TTS setup and configuration
‚îú‚îÄ‚îÄ GPU_OPTIMIZATION_SUMMARY.md  # GPU optimization details
‚îî‚îÄ‚îÄ README.md
```

### Architecture Highlights:
- **Modular Design**: Each component is separate and focused on specific functionality
- **Thread-Safe**: Interrupt system uses proper threading mechanisms
- **GPU Optimized**: All models prioritize GPU usage for maximum performance
- **Fallback Support**: Robust error handling with automatic fallbacks
- **Real-time Processing**: Optimized for minimal latency and instant responses

## Installation

1. Clone the repository
2. Create a virtual environment:
   ```bash
   python -m venv .venv
   .venv\Scripts\activate  # On Windows
   source .venv/bin/activate  # On Linux/Mac
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

4. Create a `.env` file with your API keys:
   ```
   OPENAI_API_KEY=your_openai_api_key_here
   SYSTEM_PROMPT=your_system_prompt_here
   OLLAMA_BASE_URL=http://localhost:11434  # Optional: if using custom Ollama setup
   ```

5. Install PyTorch with CUDA support (for GPU acceleration):
   ```bash
   # For CUDA 11.8
   pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118
   
   # For CUDA 12.1
   pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu121
   ```

## Quick Start

### Main Assistant with Real-time Speech
```bash
# Start the main assistant with hotword detection and interrupt TTS
python jarvis.py
```

**Features:**
- üéôÔ∏è Continuous listening for hotwords ("Bontle", "Jarvis", "Hi")
- üîÑ Interrupt TTS by saying hotwords during playback
- üë§ Automatic speaker identification using hotword audio
- ‚ö° GPU-accelerated CSM TTS for high-quality speech
- üöÄ Model preloading for instant responses

### Interactive Assistant (Text-based)
```bash
# Run text-based assistant with voice recognition setup
python assist.py
```

## TTS with Interrupt System

### How It Works
1. **Natural Conversation**: Say "Bontle" to start a conversation
2. **Interrupt Anytime**: Say "Bontle" again during TTS playback to interrupt and give new commands
3. **Seamless Transition**: No delays - immediate response to interruption
4. **Smart Audio Reuse**: Uses the same audio for both hotword detection and speaker identification

### Example Usage
```python
from assist import TTS_with_interrupt, interrupt_tts, is_tts_active

# Start interruptible TTS
result = TTS_with_interrupt("This is a long message that can be interrupted...")

# Check status
if result == "interrupted":
    print("TTS was interrupted by user")
elif result == "done":
    print("TTS completed successfully")
elif result == "error":
    print("TTS failed, check logs")

# Programmatically interrupt TTS
if is_tts_active():
    interrupt_tts()
```

### Testing Interrupt Functionality
```bash
# Run comprehensive interrupt tests
python test_interrupt.py
```

## Usage

### Advanced TTS System
```python
from assist import TTS_with_interrupt, TTS, csm_text_to_speech

# Interruptible TTS (recommended for real-time applications)
result = TTS_with_interrupt("Hello, this message can be interrupted.")

# Standard TTS with CSM + OpenAI fallback
TTS("Hello, this is a standard TTS message.")

# Direct CSM TTS usage
csm_text_to_speech("Hello from Sesame CSM!", play_audio=True)
```

### Real-time Speech Processing
```python
# Main real-time assistant (jarvis.py)
from RealtimeSTT import AudioToTextRecorder
import assist

# Initialize with GPU-optimized models
recorder = AudioToTextRecorder(
    model="medium.en", 
    enable_realtime_transcription=False,
    # Audio callback integration for seamless speaker ID
    on_recorded_chunk=on_audio_chunk
)

# Continuous loop with interrupt handling
while True:
    text = recorder.text()
    
    # Check for interrupt during TTS
    if assist.is_tts_active() and hotword_detected(text):
        assist.interrupt_tts()
    
    # Process new commands with speaker identification
    response = assist.ask_question_memory(text, identify_speaker=True, audio_file=captured_audio)
    
    # Use interruptible TTS
    assist.TTS_with_interrupt(response)
```

### Basic Assistant Functions
```python
from assist import ask_question_memory

# Ask a question with speaker identification
response = ask_question_memory("Hello, how are you?", identify_speaker=True)
print(response)
```

### Voice Signature Recognition

#### Using the Voice Signature Module
```python
# Import voice signature functions
from voice_signature import (
    register_new_speaker, 
    identify_current_speaker, 
    get_speaker_info,
    voice_manager
)

# Register a new speaker (will prompt for 5-second recording)
success = register_new_speaker("John Doe", duration=5)

# Identify current speaker (will prompt for 3-second recording)
speaker_name, confidence = identify_current_speaker(duration=3)
print(f"Speaker: {speaker_name} (confidence: {confidence:.2f})")

# Get information about registered speakers
print(get_speaker_info())

# Access the voice manager directly for advanced operations
speakers = voice_manager.list_registered_speakers()
```

#### Speaker-Aware Conversation
```python
from assist import ask_question_memory

# Ask question with automatic speaker identification
response = ask_question_memory("What's the weather like?", identify_speaker=True)
```

#### Manage Voice Database
```python
from assist import list_all_speakers, remove_registered_speaker, get_speaker_info

# List all registered speakers
speakers = list_all_speakers()
print(f"Registered speakers: {speakers}")

# Get speaker information
info = get_speaker_info()
print(info)

# Remove a speaker
remove_registered_speaker("John Doe")
```

### Interactive Assistant Demo
Run the main assistant with real-time speech and interrupt capabilities:

```bash
# Full-featured assistant with hotword detection
python jarvis.py
```

**Features in jarvis.py:**
- üéØ Continuous hotword detection ("Bontle", "Jarvis", "Hi")
- üîÑ Interrupt TTS playback with new hotwords
- üë§ Automatic speaker identification using captured audio
- ‚ö° GPU-accelerated model preloading
- üéôÔ∏è High-quality CSM TTS with OpenAI fallback
- üöÄ Optimized for minimal latency

```bash
# Text-based assistant with voice setup
python assist.py
```

**Features in assist.py:**
- üí¨ Text-based conversation interface
- üé§ Voice signature setup and management
- üîß Interactive voice settings menu
- üìä Speaker database management

### Voice Signature Management (Standalone)
Run the voice signature module independently for setup and testing:

```bash
python voice_signature.py
```

This launches an interactive menu with options to:
1. Register new speakers
2. Identify speakers  
3. List registered speakers
4. Remove speakers
5. Update speaker features
6. Demo conversation with speaker identification

### Command Line Options
```bash
# Run assistant with voice settings menu
python assist.py --voice-settings

# Run main assistant loop (default)
python assist.py
```

## Technical Architecture

### TTS Interrupt System
- **Thread-Safe Design**: Uses `threading.Event` for safe interrupt signaling
- **Resource Management**: Proper cleanup of audio files and pygame resources
- **Status Tracking**: Real-time monitoring of TTS playback state
- **Fallback Support**: CSM ‚Üí OpenAI TTS with interrupt support in both

### GPU Optimization
- **Model Preloading**: CSM TTS and RealtimeSTT models preloaded on GPU
- **Device Priority**: Automatic GPU detection and usage
- **Memory Management**: Efficient GPU memory usage with proper cleanup
- **Performance Monitoring**: Device usage tracking and optimization

### Real-time Audio Pipeline
```
Microphone ‚Üí RealtimeSTT ‚Üí Hotword Detection ‚Üí Speaker ID ‚Üí Conversation ‚Üí CSM TTS ‚Üí Audio Output
                              ‚Üì
                          Interrupt Signal ‚Üí Stop TTS ‚Üí Process New Command
```

### File Structure

```
Bontle/
‚îú‚îÄ‚îÄ assist.py                    # Core conversation logic with TTS interrupt
‚îú‚îÄ‚îÄ jarvis.py                    # Real-time speech processing main loop
‚îú‚îÄ‚îÄ csm_tts.py                   # Sesame CSM TTS with GPU optimization
‚îú‚îÄ‚îÄ voice_signature.py           # Voice recognition and speaker identification
‚îú‚îÄ‚îÄ tools.py                     # Utility functions and integrations
‚îú‚îÄ‚îÄ test_interrupt.py            # TTS interrupt testing framework
‚îú‚îÄ‚îÄ requirements.txt             # All dependencies
‚îú‚îÄ‚îÄ .env                         # Configuration and API keys
‚îú‚îÄ‚îÄ voice_signatures.json        # Speaker database
‚îú‚îÄ‚îÄ voice_recordings/            # Audio samples storage
‚îú‚îÄ‚îÄ TTS_INTERRUPT_GUIDE.md       # Complete interrupt documentation
‚îú‚îÄ‚îÄ CSM_SETUP_GUIDE.md          # CSM installation and setup
‚îú‚îÄ‚îÄ GPU_OPTIMIZATION_SUMMARY.md  # Performance optimization guide
‚îî‚îÄ‚îÄ README.md                    # This file
```

## Dependencies

### Core Dependencies
- `openai` - OpenAI API for fallback TTS
- `ollama` - Local AI conversation model
- `pygame` - Audio playback and mixing
- `python-dotenv` - Environment variable management

### TTS and Speech Processing
- `torch` - PyTorch for GPU acceleration
- `torchaudio` - Audio processing with PyTorch
- `transformers` - Hugging Face model loading
- `soundfile` - Audio file I/O
- `RealtimeSTT` - Real-time speech recognition

### Voice Signature Dependencies
- `numpy` - Numerical computations
- `scipy` - Signal processing and audio analysis
- `librosa` - Advanced audio feature extraction
- `scikit-learn` - Machine learning utilities
- `pyaudio` - Audio recording and streaming

### Optional Dependencies
- `accelerate` - Hugging Face model acceleration
- `optimum` - Model optimization
- `onnxruntime` - ONNX runtime for optimized inference

## Configuration

### Environment Variables
Create a `.env` file with:
```env
# Required
OPENAI_API_KEY=your_openai_api_key
SYSTEM_PROMPT=Your custom system prompt for conversation context

# Optional
OLLAMA_BASE_URL=http://localhost:11434
HF_HOME=./models  # Hugging Face model cache directory
CUDA_VISIBLE_DEVICES=0  # GPU device selection
```

### TTS Configuration
Modify settings in `assist.py`:
```python
# TTS Settings
USE_LOCAL_CSM_TTS = True  # Use CSM as primary, OpenAI as fallback

# Interrupt System Settings
tts_interrupt_flag = threading.Event()  # Thread-safe interrupt signal
tts_playback_active = threading.Event()  # Playback state tracking
```

### CSM TTS Settings
Modify settings in `csm_tts.py`:
```python
# Model Configuration
MODEL_NAME = "facebook/fastspeech2-en-ljspeech"  # CSM model
SAMPLE_RATE = 22050  # Audio sample rate
DEVICE_PRIORITY = ["cuda", "cpu"]  # Device selection priority

# Performance Settings
PRELOAD_MODEL = True  # Preload model for instant generation
GPU_MEMORY_FRACTION = 0.8  # GPU memory allocation
```

### Voice Signature Settings
Modify settings in `voice_signature.py`:
```python
# Audio Recording Parameters
SAMPLE_RATE = 16000      # Sample rate for recordings
CHANNELS = 1             # Mono audio
REGISTRATION_DURATION = 5 # Seconds for speaker registration
IDENTIFICATION_DURATION = 3 # Seconds for speaker identification
SIMILARITY_THRESHOLD = 0.7 # Minimum confidence for positive ID

# Feature Extraction Settings
N_MFCC = 40             # Number of MFCC coefficients
N_FFT = 2048            # FFT window size
HOP_LENGTH = 512        # Hop length for analysis
```

### RealtimeSTT Settings
Modify settings in `jarvis.py`:
```python
# Speech Recognition Configuration
recorder = AudioToTextRecorder(
    model="medium.en",  # Whisper model size
    language="en",      # Language code
    post_speech_silence_duration=0.15,  # Silence detection
    silero_sensitivity=0.4,  # Voice activity detection
    enable_realtime_transcription=False  # Disable real-time for better accuracy
)

# Hotword Configuration
hot_words = ["bontle", "jarvis", "hi"]  # Activation words
```

## Advanced Usage

### Custom TTS Integration
```python
from csm_tts import CSMTextToSpeech

# Direct CSM TTS usage with custom settings
tts = CSMTextToSpeech()
audio = tts.generate_audio("Custom text", temperature=0.9)
tts.play_audio(audio)
```

### Interrupt System Integration
```python
import assist
import threading
import time

def background_tts():
    """Run TTS in background thread"""
    result = assist.TTS_with_interrupt("This is a long message...")
    return result

# Start TTS in background
tts_thread = threading.Thread(target=background_tts)
tts_thread.start()

# Interrupt after delay
time.sleep(2)
if assist.is_tts_active():
    assist.interrupt_tts()
    print("TTS interrupted successfully")
```

### Real-time Speech Integration
```python
from RealtimeSTT import AudioToTextRecorder
import assist

def on_audio_chunk(chunk):
    """Capture audio for speaker identification"""
    global captured_audio
    captured_audio = chunk

def main_loop():
    recorder = AudioToTextRecorder(on_recorded_chunk=on_audio_chunk)
    
    while True:
        text = recorder.text()
        
        # Check for interrupt during TTS
        if assist.is_tts_active() and "bontle" in text.lower():
            assist.interrupt_tts()
        
        # Process with speaker ID using captured audio
        if "bontle" in text.lower():
            response = assist.ask_question_memory(
                text, 
                identify_speaker=True, 
                audio_file=captured_audio
            )
            assist.TTS_with_interrupt(response)
```

### Voice Database Management
```python
from voice_signature import voice_manager

# Advanced voice management
speakers = voice_manager.list_registered_speakers()
for speaker in speakers:
    info = voice_manager.get_speaker_info(speaker)
    print(f"Speaker: {speaker}, Confidence: {info.get('avg_confidence', 'N/A')}")

# Update speaker features
voice_manager.update_speaker_features("John Doe")

# Batch speaker identification
results = voice_manager.batch_identify(audio_files)
```

## Troubleshooting

### TTS Issues

1. **CSM TTS Not Working**
   - Check GPU availability: `python -c "import torch; print(torch.cuda.is_available())"`
   - Verify PyTorch installation with CUDA support
   - Check internet connection for model download
   - Review logs for specific error messages

2. **TTS Interrupt Not Working**
   - Verify threading imports in assist.py
   - Check that jarvis.py calls `interrupt_tts()` properly
   - Test with `python test_interrupt.py`
   - Ensure pygame mixer is initialized

3. **Audio Playback Problems**
   - Check audio device availability
   - Verify pygame installation: `pip install pygame`
   - Test with different audio formats
   - Check system audio settings

### Speech Recognition Issues

1. **RealtimeSTT Problems**
   - Ensure microphone permissions are granted
   - Check PyAudio installation: `pip install pyaudio`
   - Verify CUDA support for Whisper models
   - Test with different Whisper model sizes

2. **Hotword Detection Issues**
   - Adjust `silero_sensitivity` in jarvis.py
   - Check microphone input levels
   - Test with different hotwords
   - Verify post_speech_silence_duration settings

### Voice Signature Issues

1. **Speaker Recognition Accuracy**
   - Use quiet recording environment
   - Ensure consistent microphone distance
   - Re-register speakers with better quality samples
   - Adjust similarity threshold (default: 0.7)

2. **Audio Recording Problems**
   - Check microphone permissions
   - Verify audio device availability
   - Test recording duration (minimum 1 second)
   - Check scipy installation for signal processing

### GPU and Performance Issues

1. **GPU Not Being Used**
   - Install CUDA-compatible PyTorch: `pip install torch --index-url https://download.pytorch.org/whl/cu118`
   - Check CUDA installation: `nvidia-smi`
   - Verify GPU memory availability
   - Check device selection in csm_tts.py

2. **Model Loading Issues**
   - Check internet connection for Hugging Face downloads
   - Verify HF_HOME directory permissions
   - Clear model cache if corrupted: `rm -rf ~/.cache/huggingface`
   - Check disk space for model storage

3. **Performance Optimization**
   - Preload models with `preload_csm_model()`
   - Use appropriate Whisper model size for your hardware
   - Adjust GPU memory allocation settings
   - Monitor system resources during operation

### Common Error Messages

**"CUDA out of memory"**
- Reduce model size or restart Python session
- Adjust GPU_MEMORY_FRACTION in csm_tts.py
- Close other GPU-using applications

**"No module named 'transformers'"**
- Install missing dependencies: `pip install transformers`
- Check virtual environment activation

**"Audio device not found"**
- Check system audio settings
- Verify microphone connection
- Test with different audio devices

**"Model download failed"**
- Check internet connection
- Verify Hugging Face access
- Try manual model download

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test voice signature functionality
5. Submit a pull request

## License

This project is licensed under the MIT License.

## Changelog

### v3.0.0 - TTS with Interrupt & Real-time Speech (September 2025)
- ‚ú® **TTS Interrupt System**: Revolutionary interruptible text-to-speech
- üéôÔ∏è **Sesame CSM TTS**: Local, GPU-accelerated high-quality TTS
- üîÑ **Real-time Integration**: Seamless RealtimeSTT integration with audio reuse
- ‚ö° **GPU Optimization**: All models preloaded on GPU for instant responses
- üéØ **Hotword Detection**: Continuous listening with interrupt capability
- üßµ **Thread-Safe Design**: Robust interrupt handling with proper cleanup
- üìä **Smart Fallback**: CSM ‚Üí OpenAI TTS with interrupt support in both
- üöÄ **Performance**: Minimal latency with optimized model loading

### v2.0.0 - Voice Signature Recognition (Previous)
- üë§ Added complete voice signature recognition system
- üé§ Speaker registration and identification
- üí¨ Speaker-aware conversation tracking
- üîß Interactive demo and management tools
- üìà Comprehensive voice feature extraction
- üìÅ Voice database management

### v1.0.0 - Initial Release
- ü§ñ Basic AI assistant functionality
- üó£Ô∏è Text-to-Speech integration
- üí≠ Ollama conversation support

## Performance Benchmarks

### TTS Response Times (GPU)
- **CSM TTS Generation**: ~0.5-1.0 seconds
- **OpenAI TTS Fallback**: ~1.0-2.0 seconds
- **Interrupt Latency**: <0.1 seconds
- **Model Preload Time**: ~3-5 seconds (one-time)

### Speech Recognition Performance
- **Hotword Detection**: <0.2 seconds
- **RealtimeSTT Processing**: Real-time with minimal delay
- **Speaker Identification**: ~0.3-0.5 seconds
- **Audio Chunk Reuse**: No additional delay

### System Requirements
- **Minimum**: 8GB RAM, Intel i5 or equivalent
- **Recommended**: 16GB RAM, NVIDIA GPU with 4GB+ VRAM
- **Optimal**: 32GB RAM, NVIDIA RTX 3080+ or better
- **Storage**: 2-5GB for models (cached locally)

## Documentation

### Complete Guides
- üìñ **[TTS_INTERRUPT_GUIDE.md](TTS_INTERRUPT_GUIDE.md)** - Complete TTS interrupt implementation guide
- üîß **[CSM_SETUP_GUIDE.md](CSM_SETUP_GUIDE.md)** - Sesame CSM setup and configuration
- ‚ö° **[GPU_OPTIMIZATION_SUMMARY.md](GPU_OPTIMIZATION_SUMMARY.md)** - Performance optimization guide
- üìù **[README.md](README.md)** - This comprehensive overview

### Quick Reference
- üéôÔ∏è **Main Assistant**: `python jarvis.py` (real-time speech with interrupts)
- üí¨ **Text Assistant**: `python assist.py` (text-based with voice setup)
- üß™ **Test Interrupts**: `python test_interrupt.py` (verify interrupt functionality)
- üîß **Voice Setup**: `python assist.py --voice-settings` (voice management)

### API Reference
- **`assist.TTS_with_interrupt(text)`** - Interruptible TTS with status return
- **`assist.interrupt_tts()`** - Immediately stop active TTS playback
- **`assist.is_tts_active()`** - Check if TTS is currently playing
- **`csm_text_to_speech(text, play_audio=True)`** - Direct CSM TTS usage
- **`voice_manager.identify_speaker(audio_path)`** - Speaker identification

## Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Make your changes with proper documentation
4. Test thoroughly, especially TTS interrupt functionality
5. Run test suite: `python test_interrupt.py`
6. Commit changes: `git commit -m 'Add amazing feature'`
7. Push to branch: `git push origin feature/amazing-feature`
8. Submit a pull request

### Development Guidelines
- Follow existing code style and patterns
- Add comprehensive docstrings for new functions
- Test interrupt functionality thoroughly
- Update documentation for new features
- Maintain backward compatibility where possible

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Support & Contact

- üêõ **Issues**: Report bugs via GitHub Issues
- üí° **Feature Requests**: Submit enhancement ideas
- üìß **Contact**: Reach out via GitHub or email
- üìö **Documentation**: Check the complete guides in the repository

## Acknowledgments

- **Hugging Face**: For the Sesame CSM TTS model
- **OpenAI**: For GPT models and TTS API
- **RealtimeSTT**: For real-time speech recognition
- **Community**: For feedback and contributions

---

**Bontle v3.0.0** - Advanced AI Assistant with Revolutionary Interrupt TTS System

*Experience natural, interruptible conversations with your AI assistant!* üöÄ
